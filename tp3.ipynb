{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.pipeline.tagger import Tagger\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_sentences(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    modified_text = \"\"\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        modified_text += sent.text.lower()+ \" \" \n",
    "\n",
    "    return modified_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    modified_tokens = [token.text for token in doc if not token.is_punct]\n",
    "\n",
    "    modified_text = \" \".join(modified_tokens)\n",
    "\n",
    "    return modified_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    modified_tokens = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "    modified_text = \" \".join(modified_tokens)\n",
    "\n",
    "    return modified_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"USTHB is an Algerian university\"\n",
    "sent1 = lowercase_sentences(sent1)\n",
    "# sent1 = remove_punctuation(sent1)\n",
    "# sent1 = remove_stopwords(sent1)\n",
    "sent2 = \"LRIA is a research institute\"\n",
    "sent2 = lowercase_sentences(sent2)\n",
    "# sent2 = remove_punctuation(sent2)\n",
    "# sent2 = remove_stopwords(sent2)\n",
    "sent3 = \"Cars are fast\"\n",
    "sent3 = lowercase_sentences(sent3)\n",
    "sent4 = \"Cats and pillows\"\n",
    "sent4 = lowercase_sentences(sent4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usthb\n",
      "Saved model to C:\\Users\\Lenovo\\Documents\\M1S1\\TAL\\TP\n",
      "Loading from C:\\Users\\Lenovo\\Documents\\M1S1\\TAL\\TP\n",
      "Tags [('usthb', 'ORG')]\n",
      "cat cars fast\n",
      "Saved model to C:\\Users\\Lenovo\\Documents\\M1S1\\TAL\\TP\n",
      "Loading from C:\\Users\\Lenovo\\Documents\\M1S1\\TAL\\TP\n",
      "Tags [('cat', 'NNS'), ('cars', 'V'), ('fast', 'J')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define a mapping from string labels to unique integer values\n",
    "TAG_MAP = {\n",
    "    'N': {'pos': 'NOUN'},\n",
    "    'V': {'pos': 'VERB'},\n",
    "    'J': {'pos': 'ADJ'},\n",
    "    'ORG': {'pos': 'ORG'},\n",
    "    'NNS': {'pos': 'NOUN', 'number': 'plural'}  # New tag for plural nouns\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "TRAIN_DATA = [\n",
    "    (\"I like green eggs\", {'tags': ['N', 'V', 'J', 'N']}),\n",
    "    (\"Eat blue ham\", {'tags': ['V', 'J', 'N']}),\n",
    "    (sent1, {'tags': ['ORG', 'V', 'N', 'J', 'N']}),\n",
    "    (sent2, {'tags': ['ORG', 'V','N', 'N', 'N']}),\n",
    "    (sent3, {'tags': ['NNS', 'V', 'J']}),\n",
    "    (sent4, {'tags': ['NNS', 'N', 'NNS']})\n",
    "]\n",
    "\n",
    "def train_pos_tagger(lang='en', output_dir=None, n_iter=40):\n",
    "    nlp = spacy.blank(lang)\n",
    "    \n",
    "    # Create the tagger and add it to the pipeline with the name 'tagger'\n",
    "    tagger = nlp.add_pipe('tagger')\n",
    "    \n",
    "    # Add custom labels to the tagger\n",
    "    for tag in TAG_MAP.keys():\n",
    "        tagger.add_label(tag)\n",
    "        \n",
    "    \n",
    "    optimizer = nlp.begin_training()\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        \n",
    "        for text, annotations in TRAIN_DATA:\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, {\"tags\": annotations['tags']})\n",
    "            nlp.update([example], drop=0.5, losses=losses)\n",
    "        \n",
    "        # print(losses)\n",
    "    \n",
    "    \n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~ first example\n",
    "    test_text = \"i go to USTHB\"\n",
    "    test_text = lowercase_sentences(test_text)\n",
    "    test_text = remove_punctuation(test_text)\n",
    "    test_text = remove_stopwords(test_text)\n",
    "    doc = nlp(test_text)\n",
    "    print(test_text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # print('Tags', [(t.text, t.tag_) for t in doc])\n",
    "    \n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        \n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    \n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)\n",
    "    \n",
    "    # Test the saved model\n",
    "    print(\"Loading from\", output_dir)\n",
    "    nlp2 = spacy.load(output_dir)\n",
    "    doc = nlp2(test_text)\n",
    "    print('Tags', [(t.text, t.tag_) for t in doc])\n",
    "    \n",
    "    \n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~ second example\n",
    "    test_text = \"a cat and cars are fast\"\n",
    "    test_text = lowercase_sentences(test_text)\n",
    "    test_text = remove_punctuation(test_text)\n",
    "    test_text = remove_stopwords(test_text)\n",
    "    doc = nlp(test_text)\n",
    "    print(test_text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # print('Tags', [(t.text, t.tag_) for t in doc])\n",
    "    \n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        \n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    \n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)\n",
    "    \n",
    "    # Test the saved model\n",
    "    print(\"Loading from\", output_dir)\n",
    "    nlp2 = spacy.load(output_dir)\n",
    "    doc = nlp2(test_text)\n",
    "    print('Tags', [(t.text, t.tag_) for t in doc])\n",
    "\n",
    "train_pos_tagger(output_dir=\"C:/Users/Lenovo/Documents/M1S1/TAL/TP\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our own NER Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# training data\n",
    "TRAIN_DATA = [\n",
    "('Who is Shaka Khan?', {\n",
    "'entities': [(7, 17, 'PERSON')]\n",
    "}),\n",
    "('I like London and Berlin.', {\n",
    "'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')]\n",
    "})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Shaka Khan', 'PERSON')]\n",
      "Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3), ('Khan', 'PERSON', 1), ('?', '', 2)]\n",
      "Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
      "Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3), ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]\n",
      "Saved model to output_model\n",
      "Loading from output_model\n",
      "Entities [('Shaka Khan', 'PERSON')]\n",
      "Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3), ('Khan', 'PERSON', 1), ('?', '', 2)]\n",
      "Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
      "Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3), ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "\n",
    "# training data\n",
    "TRAIN_DATA = [\n",
    "    (\"Who is Shaka Khan?\", {\"entities\": [(7, 17, \"PERSON\")]}),\n",
    "    (\"I like London and Berlin.\", {\"entities\": [(7, 13, \"LOC\"), (18, 24, \"LOC\")]}),\n",
    "]\n",
    "\n",
    "# Initialize a spaCy model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create the built-in pipeline components and add them to the pipeline\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    # ner = spacy.pipeline.ner.EntityRecognizer(nlp.vocab)\n",
    "    # nlp.add_pipe(ner, last=True)\n",
    "    ner = nlp.add_pipe('ner', last=True)\n",
    "\n",
    "# Add labels\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "# Get names of other pipes to disable them during training\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "# Only train NER\n",
    "with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "    # Show warnings for misaligned entity spans once\n",
    "    warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "\n",
    "    # Reset and initialize the weights randomly\n",
    "    nlp.begin_training()\n",
    "    for itn in range(100):  # Specify the number of training iterations\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        # Batch up the examples using spaCy's minibatch\n",
    "        examples = []\n",
    "        for text, annotations in TRAIN_DATA:\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            examples.append(example)\n",
    "        nlp.update(examples, drop=0.5, losses=losses)\n",
    "        # print(\"Losses\", losses)\n",
    "\n",
    "# Test the trained model\n",
    "for text, _ in TRAIN_DATA:\n",
    "    doc = nlp(text)\n",
    "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "    print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "# Save the model to an output directory\n",
    "output_dir = Path(\"output_model\")\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir()\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)\n",
    "\n",
    "# Test the saved model\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp2 = spacy.load(output_dir)\n",
    "for text, _ in TRAIN_DATA:\n",
    "    doc = nlp2(text)\n",
    "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "    print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Entities in 'Do you like horses?'\n",
      "ANIMAL horses\n",
      "Saved model to output_model\n",
      "Loading from output_model\n",
      "ANIMAL horses\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "# new entity label\n",
    "LABEL = \"ANIMAL\"\n",
    "\n",
    "# training data\n",
    "TRAIN_DATA = [\n",
    "    (\n",
    "        \"Horses are too tall and they pretend to care about your feelings\",\n",
    "        {\"entities\": [(0, 6, LABEL)]},\n",
    "    ),\n",
    "    (\"Do they bite?\", {\"entities\": []}),\n",
    "    (\n",
    "        \"horses are too tall and they pretend to care about your feelings\",\n",
    "        {\"entities\": [(0, 6, LABEL)]},\n",
    "    ),\n",
    "    (\"horses pretend to care about your feelings\", {\"entities\": [(0, 6, LABEL)]}),\n",
    "    (\n",
    "        \"they pretend to care about your feelings, those horses\",\n",
    "        {\"entities\": [(48, 54, LABEL)]},\n",
    "    ),\n",
    "    (\"horses?\", {\"entities\": [(0, 6, LABEL)]}),\n",
    "]\n",
    "\n",
    "def main(output_dir, n_iter=30):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    random.seed(0)\n",
    "    \n",
    "    nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "    print(\"Created blank 'en' model\")\n",
    "    \n",
    "    # Add entity recognizer to model\n",
    "    ner = nlp.add_pipe(\"ner\", last=True)\n",
    "    ner.add_label(LABEL)  # add new entity label to entity recognizer\n",
    "\n",
    "    optimizer = nlp.begin_training()\n",
    "    \n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    \n",
    "    # only train NER\n",
    "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "        # show warnings for misaligned entity spans once\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "        \n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for batch in minibatch(TRAIN_DATA, size=sizes):\n",
    "                texts, annotations = zip(*batch)\n",
    "                example = []\n",
    "                # update the examples with the correct format\n",
    "                for i in range(len(texts)):\n",
    "                    doc = nlp.make_doc(texts[i])\n",
    "                    example.append(Example.from_dict(doc, annotations[i]))\n",
    "                nlp.update(example, drop=0.35, losses=losses)\n",
    "            # print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    test_text = \"Do you like horses?\"\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # save model to output directory\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.meta[\"name\"] = \"animal\"  # rename model\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)\n",
    "\n",
    "    # test the saved model\n",
    "    print(\"Loading from\", output_dir)\n",
    "    nlp2 = spacy.load(output_dir)\n",
    "    doc2 = nlp2(test_text)\n",
    "    for ent in doc2.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"output_model\", n_iter=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Entities in 'like blue'\n",
      "COLOR blue\n",
      "Saved model to output_model\n",
      "Loading from output_model\n",
      "COLOR blue\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "# new entity label\n",
    "LABEL = \"COLOR\"\n",
    "\n",
    "# training data\n",
    "TRAIN_DATA = [\n",
    "    (\n",
    "        \"The sky was a beautiful shade of blue on a clear summer day\",\n",
    "        {\"entities\": [(33, 37, LABEL)]},\n",
    "    ),\n",
    "    (\"She painted her room a vibrant shade of red to create a bold accent wall\", {\"entities\": [(40,43,LABEL)]}),\n",
    "    (\n",
    "        \"The leaves on the trees turned a brilliant gold in the autumn\",\n",
    "        {\"entities\": [(43, 47, LABEL)]},\n",
    "    ),\n",
    "    (\"His favorite color is green, and he loves spending time in nature\", {\"entities\": [(22, 27, LABEL)]}),\n",
    "    \n",
    "    (\"The dress she wore to the party was a stunning shade of royal blue\", {\"entities\": [(62, 66, LABEL)]})\n",
    "]\n",
    "TRAIN_DATA = [(text.lower(), annotations) for text, annotations in TRAIN_DATA]\n",
    "\n",
    "def main(output_dir, n_iter=30):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    random.seed(0)\n",
    "    \n",
    "    nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "    print(\"Created blank 'en' model\")\n",
    "    \n",
    "    # Add entity recognizer to model\n",
    "    ner = nlp.add_pipe(\"ner\", last=True)\n",
    "    ner.add_label(LABEL)  # add new entity label to entity recognizer\n",
    "\n",
    "    optimizer = nlp.begin_training()\n",
    "    \n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    \n",
    "    # only train NER\n",
    "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "        # show warnings for misaligned entity spans once\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "        \n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for batch in minibatch(TRAIN_DATA, size=sizes):\n",
    "                texts, annotations = zip(*batch)\n",
    "                example = []\n",
    "                # update the examples with the correct format\n",
    "                for i in range(len(texts)):\n",
    "                    doc = nlp.make_doc(texts[i])\n",
    "                    example.append(Example.from_dict(doc, annotations[i]))\n",
    "                nlp.update(example, drop=0.35, losses=losses)\n",
    "            # print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    test_text = remove_punctuation(remove_stopwords(\"Do you like blue?\")).lower()\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # save model to output directory\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.meta[\"name\"] = \"color\"  # rename model\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)\n",
    "\n",
    "    # test the saved model\n",
    "    print(\"Loading from\", output_dir)\n",
    "    nlp2 = spacy.load(output_dir)\n",
    "    doc2 = nlp2(test_text)\n",
    "    for ent in doc2.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"output_model\", n_iter=50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
